{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#train_test_split is now in model_selection.\n",
    "import os,cv2\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.model_selection import train_test_split \n",
    " \n",
    "from keras import backend as K \n",
    "K.set_image_dim_ordering('th') \n",
    " \n",
    "from keras.utils import np_utils \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten \n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D \n",
    "from keras.optimizers import SGD,RMSprop,Adam \n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows=300 \n",
    "img_cols=300 \n",
    "num_channel=1 \n",
    "num_epoch=30 \n",
    " \n",
    "# Define the number of classes \n",
    "num_classes = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-Ulos Bintang Maratur\n",
      "\n",
      "Loaded the images of dataset-Ulos Harungguan\n",
      "\n",
      "Loaded the images of dataset-Ulos Mangiring\n",
      "\n",
      "Loaded the images of dataset-Ulos Ragi Hidup\n",
      "\n",
      "Loaded the images of dataset-Ulos Ragi Hotang\n",
      "\n",
      "Loaded the images of dataset-Ulos Sadum\n",
      "\n",
      "Loaded the images of dataset-Ulos Sibolang\n",
      "\n",
      "Loaded the images of dataset-Ulos Sitolutuho\n",
      "\n",
      "(604, 300, 300)\n"
     ]
    }
   ],
   "source": [
    "Path = os.getcwd()\n",
    "#num_classes = 8\n",
    "dataset_path = \"C:/Users/TA-D3TI-06 2018/Documents/Python Scripts/Dataset Ulos\"\n",
    "data = os.listdir(dataset_path)\n",
    "img_data_list=[] \n",
    "\n",
    "for folder_dataset in data:\n",
    "#     img_list = os.listdir(dataset_path+\"/\"+folder_dataset)\n",
    "    img_list=os.listdir(dataset_path+\"/\"+ folder_dataset)  \n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(folder_dataset))  \n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(dataset_path + \"/\"  + folder_dataset + \"/\" + img)\n",
    "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        input_img_resize=cv2.resize(input_img,(img_rows,img_cols))   \n",
    "        img_data_list.append(input_img_resize) \n",
    " \n",
    "img_data = np.array(img_data_list) \n",
    "img_data = img_data.astype('float32') \n",
    "img_data /= 255\n",
    "print (img_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 22 22 ... 11 10  9]\n",
      " [21 21 20 ... 10 10 10]\n",
      " [21 21 20 ... 13 13 12]\n",
      " ...\n",
      " [10 10 10 ...  6  6  5]\n",
      " [10 11 12 ...  7  6  6]\n",
      " [ 9  9 10 ...  7  7  7]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 1, 300, 300)\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  10  11  12  13  14  16  17  19  20\n",
      "  21  22  23  24  25  27  29  30  31  32  33  34  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  58  59  60  61\n",
      "  62  63  65  66  67  68  69  70  71  72  73  75  76  77  78  79  80  81\n",
      "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 115 116 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
      " 160 162 163 164 165 167 168 170 171 172 173 174 175 176 177 178 181 182\n",
      " 183 184 185 186 188 190 191 192 193 194 195 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 221 222\n",
      " 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n",
      " 241 242 243 244 246 247 248 249 250 251 252 253 254 257 258 259 260 261\n",
      " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
      " 280 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 300 301 302 303 304 305 306 307 309 310 311 312 313 314 317 318 319 320\n",
      " 321 322 323 324 327 328 329 330 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 365 366 368 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 416 417\n",
      " 418 419 420 421 422 423 424 425 426 428 430 431 432 433 434 436 438 440\n",
      " 441 442 443 445 446 447 448 450 451 452 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 469 470 471 472 473 474 475 476 477 478 479\n",
      " 480 481 482 483 484 486 487 489 490 491 492 493 494 495 496 497 498 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 515 516 517 518 519\n",
      " 520 521 522 523 524 526 527 528 529 530 531 532 533 534 535 536 537 539\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 555 556 557 558 559\n",
      " 560 561 562 563 564 565 566 567 568 570 571 572 573 574 575 576 577 578\n",
      " 579 580 581 583 584 585 587 589 590 591 592 593 595 596 597 598 599 600\n",
      " 601 602 603] \n",
      "TEST: [  3  15  18  26  28  35  56  57  64  74 100 101 114 117 134 159 161 166\n",
      " 169 179 180 187 189 196 220 245 255 256 281 299 308 315 316 325 326 331\n",
      " 364 367 369 415 427 429 435 437 439 444 449 468 485 488 499 514 525 538\n",
      " 540 554 569 582 586 588 594]\n",
      "WARNING:tensorflow:From C:\\Users\\TA-D3TI-06 2018\\Anaconda3\\envs\\TugasAkhir\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36\n",
      "  37  38  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55\n",
      "  56  57  58  59  60  61  62  63  64  65  66  67  68  70  71  72  73  74\n",
      "  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  93\n",
      "  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 110 111 112\n",
      " 113 114 115 116 117 118 119 120 121 122 123 125 126 127 128 129 132 133\n",
      " 134 135 136 138 139 140 142 143 144 145 146 147 148 149 150 152 153 154\n",
      " 155 156 157 159 160 161 162 163 164 165 166 167 168 169 172 173 174 175\n",
      " 176 177 178 179 180 181 182 183 185 186 187 188 189 190 191 192 194 195\n",
      " 196 197 198 199 200 201 202 203 204 205 207 208 209 210 211 212 213 214\n",
      " 215 216 217 218 219 220 221 222 223 224 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 251 253\n",
      " 254 255 256 258 259 260 261 262 263 264 265 266 267 268 269 270 271 273\n",
      " 274 276 277 278 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 297 298 299 300 301 302 303 304 305 306 307 308 310 311 312 313\n",
      " 314 315 316 317 318 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 359 360 363 364 365 366 367 368 369 370 371\n",
      " 372 373 375 376 377 378 379 380 382 383 384 385 386 387 388 389 390 391\n",
      " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
      " 410 411 412 413 414 415 417 418 420 421 422 423 424 425 427 428 429 430\n",
      " 431 432 434 435 436 437 438 439 440 443 444 445 446 447 448 449 450 452\n",
      " 454 455 456 457 458 459 460 461 462 464 465 466 467 468 469 470 471 472\n",
      " 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491\n",
      " 492 493 495 496 497 498 499 500 502 504 505 506 507 508 509 510 512 513\n",
      " 514 515 517 518 520 521 522 523 524 525 526 527 528 529 531 532 533 534\n",
      " 536 537 538 539 540 541 542 543 545 546 548 549 550 551 552 553 554 555\n",
      " 556 557 559 561 562 564 565 567 568 569 570 572 573 574 575 576 577 578\n",
      " 579 580 582 583 584 585 586 587 588 589 590 592 593 594 595 596 597 598\n",
      " 600 602 603] \n",
      "TEST: [ 33  43  69  92 109 124 130 131 137 141 151 158 170 171 184 193 206 225\n",
      " 250 252 257 272 275 279 296 309 319 358 361 362 374 381 416 419 426 433\n",
      " 441 442 451 453 463 473 494 501 503 511 516 519 530 535 544 547 558 560\n",
      " 563 566 571 581 591 599 601]\n",
      "TRAIN: [  1   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
      "  21  22  24  26  27  28  30  32  33  35  36  37  38  39  40  41  42  43\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  60  61  62  63\n",
      "  64  66  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  85\n",
      "  86  87  88  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 106 107 108 109 111 112 113 114 115 116 117 118 119 120 121 122 124 125\n",
      " 126 127 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 191 192 193 194 195 196 197 198 199 201 202\n",
      " 203 204 205 206 207 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
      " 222 223 224 225 226 227 228 229 230 231 232 233 235 237 239 240 241 242\n",
      " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
      " 261 262 263 266 267 268 270 271 272 273 275 276 277 278 279 280 281 282\n",
      " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 301 303 304\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 321 322 324 325\n",
      " 326 327 328 329 330 331 332 333 335 336 337 338 339 340 341 342 343 344\n",
      " 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 365 366 367 368 369 370 371 372 373 374 376 378 379 381 382 383 384\n",
      " 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 401 402 403\n",
      " 404 405 406 407 409 410 411 412 413 414 415 416 417 419 420 422 423 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 436 437 439 440 441 442 443\n",
      " 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461\n",
      " 462 463 464 465 467 468 469 471 472 473 474 475 476 477 478 479 480 481\n",
      " 482 483 484 485 487 488 489 490 491 493 494 495 496 497 498 499 500 501\n",
      " 502 503 504 505 506 508 509 511 513 514 515 516 517 518 519 521 522 523\n",
      " 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541\n",
      " 542 543 544 545 546 547 548 549 550 551 552 553 554 555 557 558 559 560\n",
      " 562 563 564 565 566 567 568 569 570 571 572 573 574 575 577 578 579 580\n",
      " 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 597 598 599\n",
      " 600 601 603] \n",
      "TEST: [  0   2  16  23  25  29  31  34  44  59  65  67  68  84  89  90 110 123\n",
      " 128 129 150 190 200 208 234 236 238 264 265 269 274 283 284 300 302 305\n",
      " 320 323 334 345 375 377 380 400 408 418 421 438 466 470 486 492 507 510\n",
      " 512 520 556 561 576 596 602]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  63  64  65  67  68  69  70  72  73  74\n",
      "  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  92  93  95\n",
      "  96  97  99 100 101 102 103 104 105 106 108 109 110 111 112 114 115 117\n",
      " 118 119 120 121 122 123 124 125 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 161 162 164 165 166 167 168 169 170 171 172 173 175 176\n",
      " 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194\n",
      " 195 196 197 199 200 201 202 203 205 206 207 208 209 211 212 214 215 216\n",
      " 217 218 220 221 222 223 224 225 226 227 228 229 230 231 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 248 249 250 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 274 275\n",
      " 276 277 279 280 281 283 284 285 286 288 289 290 291 293 294 295 296 297\n",
      " 299 300 301 302 303 304 305 306 307 308 309 310 314 315 316 317 318 319\n",
      " 320 321 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      " 339 340 341 342 344 345 346 347 349 350 351 352 353 354 355 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 389 391 392 394 396 397 398 400\n",
      " 401 402 405 407 408 409 410 411 412 413 414 415 416 417 418 419 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 441\n",
      " 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 458 459 460\n",
      " 461 462 463 464 466 467 468 469 470 471 472 473 475 476 477 478 479 480\n",
      " 481 482 483 484 485 486 487 488 489 490 491 492 493 494 496 497 498 499\n",
      " 500 501 502 503 505 506 507 508 509 510 511 512 513 514 516 517 518 519\n",
      " 520 522 523 525 526 528 530 531 533 534 535 536 538 539 540 541 542 543\n",
      " 544 545 546 547 548 549 550 551 552 553 554 556 557 558 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 580 581 582\n",
      " 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600\n",
      " 601 602 603] \n",
      "TEST: [ 62  66  71  75  91  94  98 107 113 116 126 147 160 163 174 198 204 210\n",
      " 213 219 232 247 251 273 278 282 287 292 298 311 312 313 322 343 348 371\n",
      " 388 390 393 395 399 403 404 406 420 440 457 465 474 495 504 515 521 524\n",
      " 527 529 532 537 555 559 579]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  18  20\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  41\n",
      "  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  81  82  84  85  86  87  88  89  90  91  92  93  94  95  97  98 100\n",
      " 101 103 104 105 106 107 108 109 110 112 113 114 116 117 118 120 121 122\n",
      " 123 124 125 126 127 128 129 130 131 132 133 134 135 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157 158 159 160\n",
      " 161 163 164 165 166 167 168 169 170 171 173 174 175 176 177 178 179 180\n",
      " 181 183 184 185 186 187 189 190 191 192 193 194 195 196 198 200 202 203\n",
      " 204 205 206 207 208 209 210 211 213 215 216 217 218 219 220 221 222 223\n",
      " 224 225 227 228 229 230 231 232 233 234 235 236 238 240 241 242 243 244\n",
      " 245 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 264\n",
      " 265 266 267 268 269 270 271 272 273 274 275 276 278 279 280 281 282 283\n",
      " 284 286 287 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 336 338 339 340 341 342\n",
      " 343 345 346 347 348 349 352 353 354 355 357 358 359 360 361 362 363 364\n",
      " 366 367 368 369 370 371 372 374 375 376 377 378 379 380 381 382 384 385\n",
      " 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 402 403 404\n",
      " 405 406 408 409 411 412 414 415 416 417 418 419 420 421 422 423 424 425\n",
      " 426 427 428 429 430 431 432 433 434 435 437 438 439 440 441 442 443 444\n",
      " 446 447 449 450 451 452 453 454 455 456 457 458 459 460 463 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n",
      " 505 506 507 509 510 511 512 513 514 515 516 517 518 519 520 521 522 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 540 541 542 543\n",
      " 544 546 547 548 549 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581\n",
      " 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599\n",
      " 600 601 602 603] \n",
      "TEST: [ 13  17  19  21  37  40  50  80  83  96  99 102 111 115 119 136 152 162\n",
      " 172 182 188 197 199 201 212 214 226 237 239 246 263 277 285 288 304 335\n",
      " 337 344 350 351 356 365 373 383 401 407 410 413 436 445 448 461 462 464\n",
      " 490 508 523 539 545 550]\n",
      "TRAIN: [  0   1   2   3   4   5   6   8  10  11  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  28  29  30  31  32  33  34  35  36  37  38  40\n",
      "  41  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  77  79\n",
      "  80  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
      "  99 100 101 102 103 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 140 141 142 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 166 167 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 204 206 207 208 210 211 212 213 214 215 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 241 242 244 245 246 247 248 249 250 251 252 253 254 255 256 257 259\n",
      " 260 262 263 264 265 266 267 269 270 271 272 273 274 275 276 277 278 279\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
      " 337 338 340 342 343 344 345 346 348 349 350 351 352 354 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 385 386 387 388 389 390 391 392 393 394 395 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 410 411 412 413 414 415 416 417\n",
      " 418 419 420 421 422 423 424 425 426 427 429 430 433 434 435 436 437 438\n",
      " 439 440 441 442 443 444 445 446 448 449 450 451 452 453 454 455 456 457\n",
      " 458 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476\n",
      " 477 478 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495\n",
      " 496 497 498 499 501 503 504 505 506 507 508 509 510 511 512 514 515 516\n",
      " 519 520 521 522 523 524 525 527 528 529 530 531 532 533 535 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 568 569 570 571 572 573 574 576 577\n",
      " 578 579 580 581 582 583 584 585 586 587 588 590 591 592 594 595 596 598\n",
      " 599 601 602 603] \n",
      "TEST: [  7   9  12  27  39  45  70  78  88 104 105 139 143 144 145 146 165 168\n",
      " 185 203 205 209 216 240 243 258 261 268 280 294 336 339 341 347 353 355\n",
      " 368 384 396 409 428 431 432 447 459 479 500 502 513 517 518 526 534 536\n",
      " 567 575 589 593 597 600]\n",
      "TRAIN: [  0   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  47  48  50  51  52  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  74  75  76  77  78  80  81\n",
      "  82  83  84  85  86  88  89  90  91  92  93  94  96  97  98  99 100 101\n",
      " 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 127 128 129 130 131 134 135 136 137 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 177 179\n",
      " 180 182 184 185 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 232 233 234 235 236 237 238\n",
      " 239 240 242 243 244 245 246 247 248 249 250 251 252 253 255 256 257 258\n",
      " 259 261 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278\n",
      " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 296 297\n",
      " 298 299 300 301 302 304 305 308 309 311 312 313 314 315 316 317 318 319\n",
      " 320 321 322 323 324 325 326 327 328 331 332 333 334 335 336 337 338 339\n",
      " 341 342 343 344 345 346 347 348 349 350 351 353 354 355 356 357 358 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 379\n",
      " 380 381 382 383 384 385 387 388 389 390 392 393 394 395 396 397 398 399\n",
      " 400 401 403 404 405 406 407 408 409 410 413 415 416 417 418 419 420 421\n",
      " 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      " 459 460 461 462 463 464 465 466 467 468 469 470 471 473 474 475 476 478\n",
      " 479 481 485 486 487 488 489 490 491 492 493 494 495 496 497 499 500 501\n",
      " 502 503 504 505 507 508 509 510 511 512 513 514 515 516 517 518 519 520\n",
      " 521 522 523 524 525 526 527 528 529 530 531 532 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 563 564 565 566 567 568 569 571 573 574 575 576 577 579\n",
      " 580 581 582 583 584 586 587 588 589 590 591 592 593 594 596 597 598 599\n",
      " 600 601 602 603] \n",
      "TEST: [  1   8  30  46  49  53  54  72  73  79  87  95 106 132 133 138 176 178\n",
      " 181 183 186 218 231 241 254 260 267 295 303 306 307 310 329 330 340 352\n",
      " 359 378 386 391 402 411 412 414 422 472 477 480 482 483 484 498 506 533\n",
      " 562 570 572 578 585 595]\n",
      "TRAIN: [  0   1   2   3   4   6   7   8   9  10  12  13  14  15  16  17  18  19\n",
      "  21  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37  39  40\n",
      "  41  43  44  45  46  48  49  50  51  52  53  54  56  57  59  61  62  64\n",
      "  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82\n",
      "  83  84  85  87  88  89  90  91  92  93  94  95  96  98  99 100 101 102\n",
      " 103 104 105 106 107 109 110 111 113 114 115 116 117 118 119 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 150 151 152 156 158 159 160 161 162 163 164 165\n",
      " 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 192 193 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 208 209 210 212 213 214 215 216 217 218 219 220 221 223 224\n",
      " 225 226 227 228 231 232 233 234 235 236 237 238 239 240 241 243 244 245\n",
      " 246 247 250 251 252 253 254 255 256 257 258 259 260 261 263 264 265 266\n",
      " 267 268 269 270 272 273 274 275 276 277 278 279 280 281 282 283 284 285\n",
      " 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
      " 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321\n",
      " 322 323 325 326 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 343 344 345 347 348 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400\n",
      " 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418\n",
      " 419 420 421 422 425 426 427 428 429 430 431 432 433 434 435 436 437 438\n",
      " 439 440 441 442 443 444 445 446 447 448 449 451 453 454 455 456 457 458\n",
      " 459 461 462 463 464 465 466 467 468 470 471 472 473 474 476 477 479 480\n",
      " 481 482 483 484 485 486 487 488 490 491 492 493 494 495 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 523 524 525 526 527 528 529 530 532 533 534 535 536 537 538\n",
      " 539 540 541 542 543 544 545 546 547 549 550 551 552 553 554 555 556 558\n",
      " 559 560 561 562 563 564 566 567 568 569 570 571 572 573 574 575 576 578\n",
      " 579 580 581 582 584 585 586 588 589 590 591 592 593 594 595 596 597 598\n",
      " 599 600 601 602] \n",
      "TEST: [  5  11  20  32  38  42  47  55  58  60  63  86  97 108 112 120 121 149\n",
      " 153 154 155 157 191 194 207 211 222 229 230 242 248 249 262 271 324 327\n",
      " 342 346 349 366 382 423 424 450 452 460 469 475 478 489 496 522 531 548\n",
      " 557 565 577 583 587 603]\n",
      "TRAIN: [  0   1   2   3   5   6   7   8   9  11  12  13  15  16  17  18  19  20\n",
      "  21  23  25  26  27  28  29  30  31  32  33  34  35  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  50  53  54  55  56  57  58  59  60  62\n",
      "  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78  79  80  81\n",
      "  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 123 124 126 127 128 129 130 131 132 133 134 136 137 138 139\n",
      " 141 143 144 145 146 147 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 165 166 167 168 169 170 171 172 174 176 177 178 179 180 181\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
      " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 254 255\n",
      " 256 257 258 260 261 262 263 264 265 267 268 269 271 272 273 274 275 277\n",
      " 278 279 280 281 282 283 284 285 287 288 289 290 291 292 294 295 296 297\n",
      " 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 315 316\n",
      " 319 320 322 323 324 325 326 327 329 330 331 332 334 335 336 337 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 355 356 357 358 359\n",
      " 361 362 364 365 366 367 368 369 371 372 373 374 375 377 378 380 381 382\n",
      " 383 384 385 386 387 388 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420\n",
      " 421 422 423 424 425 426 427 428 429 431 432 433 435 436 437 438 439 440\n",
      " 441 442 443 444 445 447 448 449 450 451 452 453 454 456 457 459 460 461\n",
      " 462 463 464 465 466 468 469 470 471 472 473 474 475 477 478 479 480 481\n",
      " 482 483 484 485 486 487 488 489 490 491 492 494 495 496 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536\n",
      " 537 538 539 540 541 542 544 545 547 548 550 553 554 555 556 557 558 559\n",
      " 560 561 562 563 564 565 566 567 569 570 571 572 574 575 576 577 578 579\n",
      " 580 581 582 583 584 585 586 587 588 589 591 592 593 594 595 596 597 599\n",
      " 600 601 602 603] \n",
      "TEST: [  4  10  14  22  24  36  51  52  61  76  93 122 125 135 140 142 148 164\n",
      " 173 175 235 253 259 266 270 276 286 293 314 317 318 321 328 333 338 354\n",
      " 360 363 370 376 379 389 405 430 434 446 455 458 467 476 493 543 546 549\n",
      " 551 552 568 573 590 598]\n",
      "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  42  43  44  45  46  47  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
      "  75  76  78  79  80  83  84  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115\n",
      " 116 117 119 120 121 122 123 124 125 126 128 129 130 131 132 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153\n",
      " 154 155 157 158 159 160 161 162 163 164 165 166 168 169 170 171 172 173\n",
      " 174 175 176 178 179 180 181 182 183 184 185 186 187 188 189 190 191 193\n",
      " 194 196 197 198 199 200 201 203 204 205 206 207 208 209 210 211 212 213\n",
      " 214 216 218 219 220 222 225 226 229 230 231 232 234 235 236 237 238 239\n",
      " 240 241 242 243 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
      " 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276\n",
      " 277 278 279 280 281 282 283 284 285 286 287 288 292 293 294 295 296 298\n",
      " 299 300 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 333 334 335 336\n",
      " 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354\n",
      " 355 356 358 359 360 361 362 363 364 365 366 367 368 369 370 371 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 386 388 389 390 391 393 395 396\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 418 419 420 421 422 423 424 426 427 428 429 430 431 432 433 434 435 436\n",
      " 437 438 439 440 441 442 444 445 446 447 448 449 450 451 452 453 455 457\n",
      " 458 459 460 461 462 463 464 465 466 467 468 469 470 472 473 474 475 476\n",
      " 477 478 479 480 482 483 484 485 486 488 489 490 492 493 494 495 496 498\n",
      " 499 500 501 502 503 504 506 507 508 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 529 530 531 532 533 534 535 536 537\n",
      " 538 539 540 543 544 545 546 547 548 549 550 551 552 554 555 556 557 558\n",
      " 559 560 561 562 563 565 566 567 568 569 570 571 572 573 575 576 577 578\n",
      " 579 581 582 583 585 586 587 588 589 590 591 593 594 595 596 597 598 599\n",
      " 600 601 602 603] \n",
      "TEST: [  6  41  48  77  81  82  85 103 118 127 156 167 177 192 195 202 215 217\n",
      " 221 223 224 227 228 233 244 289 290 291 297 301 332 357 372 385 387 392\n",
      " 394 397 398 417 425 443 454 456 471 481 487 491 497 505 509 528 541 542\n",
      " 553 564 574 580 584 592]\n"
     ]
    }
   ],
   "source": [
    "if num_channel==1:  \n",
    "    if K.image_dim_ordering()=='th':   \n",
    "        img_data= np.expand_dims(img_data, axis=1)    \n",
    "        print (img_data.shape)  \n",
    "    else:   \n",
    "        img_data= np.expand_dims(img_data, axis=4)    \n",
    "        print (img_data.shape)    \n",
    "else:  \n",
    "    if K.image_dim_ordering()=='th':   \n",
    "        img_data=np.rollaxis(img_data,3,1)   \n",
    "        print (img_data.shape)    \n",
    "        \n",
    "# Assigning Labels \n",
    "\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "# =============================================================================\n",
    "# names = ['Ulos Sibolang', 'Ulos Sadum', 'Ulos Ragi Hidup','Ulos Mangiring',\n",
    "#'Ulos Harungguan', 'Ulos Ragi Hotang', 'Ulos Bintang Maratur', 'Ulos Sitolutuho']\n",
    "# labels[0:632]=0\n",
    "# labels[632:1440]=1\n",
    "# labels[1440:2248]=2\n",
    "# labels[2248:2912]=3\n",
    "# labels[2912:3176]=4\n",
    "# labels[3176:4272]=5\n",
    "# labels[4272:4888]=6\n",
    "# labels[4888:]=7\n",
    "# # convert class labels to on-hot encoding\n",
    "# =============================================================================\n",
    "\n",
    "names = ['Ulos Bintang Maratur', 'Ulos Ragi Hidup', 'Ulos Sitolutuho','Ulos Sibolang', \n",
    "         'Ulos Ragi Hotang', 'Ulos Sadum', 'Ulos Harungguan', 'Ulos Mangiring']\n",
    "labels[0:74]=0\n",
    "labels[75:149]=1\n",
    "labels[150:214]=2\n",
    "labels[215:303]=3\n",
    "labels[304:378]=4\n",
    "labels[379:453]=5\n",
    "labels[454:528]=6\n",
    "labels[529:603]=7\n",
    "\n",
    "Y = np_utils.to_categorical(labels, num_classes) \n",
    "#X = img_data\n",
    " \n",
    "# #Shuffle the dataset \n",
    "x,y = shuffle(img_data,Y, random_state=2) \n",
    "seed = 7 \n",
    "np.random.seed(seed) \n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) \n",
    " \n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, train_size = 0.80, test_size = 0.20, random_state = 0)\n",
    "#input_shape=img_data[0].shape \n",
    "#input_shape\n",
    "#input_shape = input_shape + (1,)\n",
    "#input_shape\n",
    "#X_train = np.asarray( x[train_index]).reshape((len(X_train),360,380,1))\n",
    "#X_Test = np.asarray(X_Test).reshape((len(X_Test),900,900,1))\n",
    "#X_Train.shape\n",
    "input_shape = img_data[0].shape\n",
    "counter = 0 \n",
    "\n",
    "for train_index, test_index in kfold.split(x,y[:,0]):      \n",
    "    print(\"TRAIN:\", train_index, \"\\nTEST:\", test_index)      \n",
    "    X_train, X_test = x[train_index], x[test_index]      \n",
    "    y_train, y_test = y[train_index], y[test_index]      \n",
    "    counter+=1\n",
    "    nilai_learning_rate=0.001\n",
    "    opt=Adam(lr=nilai_learning_rate)\n",
    "    np.savetxt('C:/Users/TA-D3TI-06 2018/Documents/Python Scripts/output original/'+str(counter)+'-train_index.csv', train_index, fmt='%d', delimiter=',')\n",
    "    np.savetxt('C:/Users/TA-D3TI-06 2018/Documents/Python Scripts/output original/'+str(counter)+'-test_index.csv', test_index, fmt='%d', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: name 'X_train' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'y_train' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'X_test' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'y_test' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'X_train' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'y_train' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'X_test' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'y_test' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'X_train' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'y_train' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'X_test' is used prior to global declaration\n",
      "<>:48: SyntaxWarning: name 'y_test' is used prior to global declaration\n",
      "<ipython-input-6-fd480407b8d3>:48: SyntaxWarning: name 'X_train' is used prior to global declaration\n",
      "  global X_train, y_train, X_test, y_test\n",
      "<ipython-input-6-fd480407b8d3>:48: SyntaxWarning: name 'y_train' is used prior to global declaration\n",
      "  global X_train, y_train, X_test, y_test\n",
      "<ipython-input-6-fd480407b8d3>:48: SyntaxWarning: name 'X_test' is used prior to global declaration\n",
      "  global X_train, y_train, X_test, y_test\n",
      "<ipython-input-6-fd480407b8d3>:48: SyntaxWarning: name 'y_test' is used prior to global declaration\n",
      "  global X_train, y_train, X_test, y_test\n"
     ]
    }
   ],
   "source": [
    "def createModel():\n",
    "    \n",
    "    model = Sequential()\n",
    "             \n",
    "    model.add(Convolution2D(32, 3, strides=(3, 3), padding='same',input_shape=input_shape))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "                 \n",
    "    model.add(Convolution2D(64, 3, strides=(3, 3)))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "                 \n",
    "    model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "                 \n",
    "    model.add(Dropout(0.5))\n",
    "                 \n",
    "    model.add(Convolution2D(128, 3, strides=(3, 3)))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "                 \n",
    "    model.add(Convolution2D(256, 3, strides=(3, 3)))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "                 \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                 \n",
    "    model.add(Dropout(0.5))\n",
    "                 \n",
    "    model.add(Flatten())\n",
    "                 \n",
    "    model.add(Dense(128))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "                 \n",
    "    model.add(Activation('relu'))\n",
    "                 \n",
    "    model.add(Dense(num_classes))\n",
    "                 \n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])    \n",
    "    hist = model.fit(X_train, y_train, epochs=num_epoch, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
    "    global X_train, y_train, X_test, y_test\n",
    "    score = model.evaluate(X_test, y_test, batch_size= 32, verbose=0)\n",
    "    model.save('C:/Users/TA-D3TI-06 2018/Documents/Python Scripts/output original/'+str(counter)+'-K-fold-model.hdf5')\n",
    "    print('K-Fold -', counter)    \n",
    "    print('==================================')\n",
    "    print('Test Loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('==================================')\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TA-D3TI-06 2018\\Anaconda3\\envs\\TugasAkhir\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\TA-D3TI-06 2018\\Anaconda3\\envs\\TugasAkhir\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 544 samples, validate on 60 samples\n",
      "Epoch 1/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 2.0745 - acc: 0.1195 - val_loss: 2.0796 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 2.0776 - acc: 0.1250 - val_loss: 2.0762 - val_acc: 0.1500\n",
      "Epoch 3/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 2.0631 - acc: 0.1581 - val_loss: 2.0784 - val_acc: 0.1500\n",
      "Epoch 4/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 2.0673 - acc: 0.1599 - val_loss: 2.0702 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 2.0182 - acc: 0.2371 - val_loss: 1.8820 - val_acc: 0.3167\n",
      "Epoch 6/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.8708 - acc: 0.2518 - val_loss: 1.6341 - val_acc: 0.3333\n",
      "Epoch 7/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.7261 - acc: 0.2629 - val_loss: 1.4792 - val_acc: 0.5167\n",
      "Epoch 8/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.5885 - acc: 0.3125 - val_loss: 1.3893 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.4478 - acc: 0.3934 - val_loss: 1.2243 - val_acc: 0.5667\n",
      "Epoch 10/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.2730 - acc: 0.4596 - val_loss: 1.2305 - val_acc: 0.5333\n",
      "Epoch 11/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.1900 - acc: 0.5202 - val_loss: 1.0075 - val_acc: 0.6833\n",
      "Epoch 12/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 1.0557 - acc: 0.6029 - val_loss: 1.1075 - val_acc: 0.5667\n",
      "Epoch 13/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.9707 - acc: 0.6158 - val_loss: 0.9454 - val_acc: 0.6667\n",
      "Epoch 14/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.9510 - acc: 0.6324 - val_loss: 1.0582 - val_acc: 0.6000\n",
      "Epoch 15/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.8510 - acc: 0.6930 - val_loss: 0.9042 - val_acc: 0.6167\n",
      "Epoch 16/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.7519 - acc: 0.7151 - val_loss: 0.8236 - val_acc: 0.6833\n",
      "Epoch 17/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.6165 - acc: 0.7665 - val_loss: 0.6395 - val_acc: 0.7833\n",
      "Epoch 18/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.5981 - acc: 0.7831 - val_loss: 0.9078 - val_acc: 0.6500\n",
      "Epoch 19/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.5803 - acc: 0.7721 - val_loss: 0.7247 - val_acc: 0.8167\n",
      "Epoch 20/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.5403 - acc: 0.7904 - val_loss: 0.7202 - val_acc: 0.7667\n",
      "Epoch 21/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.4586 - acc: 0.8327 - val_loss: 0.5876 - val_acc: 0.8333\n",
      "Epoch 22/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.4345 - acc: 0.8419 - val_loss: 0.5662 - val_acc: 0.8833\n",
      "Epoch 23/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3885 - acc: 0.8548 - val_loss: 0.4242 - val_acc: 0.8833\n",
      "Epoch 24/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3666 - acc: 0.8713 - val_loss: 0.4884 - val_acc: 0.8833\n",
      "Epoch 25/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3458 - acc: 0.8732 - val_loss: 0.4873 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3681 - acc: 0.8566 - val_loss: 0.4465 - val_acc: 0.8333\n",
      "Epoch 27/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3483 - acc: 0.8585 - val_loss: 0.4123 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3124 - acc: 0.8621 - val_loss: 0.3775 - val_acc: 0.9167\n",
      "Epoch 29/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.2821 - acc: 0.8989 - val_loss: 0.4583 - val_acc: 0.9167\n",
      "Epoch 30/30\n",
      "544/544 [==============================] - 6s 11ms/step - loss: 0.3019 - acc: 0.8787 - val_loss: 0.4288 - val_acc: 0.9000\n",
      "K-Fold - 10\n",
      "==================================\n",
      "Test Loss: 0.4287988026936849\n",
      "Test accuracy: 0.9000000119209289\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TugasAkhir",
   "language": "python",
   "name": "tugasakhir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
